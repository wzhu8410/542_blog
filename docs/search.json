[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts.\nAndy was here"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "From Data to Model: Building an End-to-End Pipeline in Python",
    "section": "",
    "text": "Pipeline is one of the most important concepts in machine learning with profound practical use. It organizes the machine learning workflow of an entire process of taking raw data, processing it, and building a model that is based off given data to be implemented on countless others. The pipeline includes more than just a model, rather, it is a Blackbox that contains all the tricks and whistles to perform a machine learning process in its entirety. If you feed raw dough into this pipeline, it would perform the necessary work to build a delicious bread from the other end.\n\n\n\nBeautiful Bread Coming Out of Oven"
  },
  {
    "objectID": "posts/post-with-code/index.html#a-pipeline-is-typically-consisted-of-the-following-steps",
    "href": "posts/post-with-code/index.html#a-pipeline-is-typically-consisted-of-the-following-steps",
    "title": "From Data to Model: Building an End-to-End Pipeline in Python",
    "section": "A pipeline is typically consisted of the following steps:",
    "text": "A pipeline is typically consisted of the following steps:\n\nData Loading\nThe first step to load the raw data into the pipeline. Raw data can be consisted of various formats (e.g. .csv, .xlsx) and from various sources (e.g. API, local). Efficient data handling is essential to get started on building a pipeline.\n\nData Preprocessing\nRaw data is often messy with repeated values, missing values, and/or invalid values. This step is crucial to clean the data as desired before feeding it to the pipeline. Preprocessing also includes basic exploratory data analysis (EDA) to understand the dataset’s structure and patterns.\nFeature Engineering\nThis steps help filter out the features that are used in the upcoming model. Features can be manipulated by many ways. We often see this step being performed with encoding, combining, and with expert knowledge.\n\nModel Training\nNow it gets to the fun part. Based on the data and questions asked, we choose an appropriate machine learning algorithm and train the model. It is crucial to split the data into training and validation sets and tune the hyperparameters to optimize model.\n\nEvaluation\nAfter training the model, we evaluate the model’s performance using multiple metrics such as accuracy, f1 score, or precision/recall. After we have a satisfied model we are ready to deploy it to unseen data."
  },
  {
    "objectID": "posts/post-with-code/index.html#tools-for-building-ml-pipelines",
    "href": "posts/post-with-code/index.html#tools-for-building-ml-pipelines",
    "title": "From Data to Model: Building an End-to-End Pipeline in Python",
    "section": "Tools for Building ML Pipelines",
    "text": "Tools for Building ML Pipelines\n\nData Loading: pandas, NumPy.\nData Preprocessing: scikit-learn, pandas, matplotlib.\nFeature Engineering: scikit-learn.\nModel Training: scikit-learn, XGBoost, PyTorch.\nEvaluation: scikit-learn, seaborn, FastAPI, Docker.\n\nThese python packages often come in handy for each of the steps within the pipeline. Specifically, scikit-learn is a machine learning packages that will get you started straightforward with models and preprocessing encoders. Matplotlib and seaborn are a few plotting packages that is useful to perform EDA to understand the basic data structure and to evaluate the model performance after training.\nAfter getting an understanding of the structure of a pipeline, we can see that a completed pipeline would be able to take in a raw dataset, clean it to be training-ready, and train the model based on the data. The model we obtained from this pipeline can make classification or regression predictions on future unseen data."
  },
  {
    "objectID": "posts/post-with-code/index.html#step-1-data-loading",
    "href": "posts/post-with-code/index.html#step-1-data-loading",
    "title": "From Data to Model: Building an End-to-End Pipeline in Python",
    "section": "Step 1: Data Loading",
    "text": "Step 1: Data Loading\nStart by loading your dataset. For this example, we’ll use a CSV file:\n\nimport pandas as pd\n\n# Load the dataset\ndata = pd.read_csv('data.csv')\nprint(data.head())"
  },
  {
    "objectID": "posts/post-with-code/index.html#step-2-data-preprocessing",
    "href": "posts/post-with-code/index.html#step-2-data-preprocessing",
    "title": "From Data to Model: Building an End-to-End Pipeline in Python",
    "section": "Step 2: Data Preprocessing",
    "text": "Step 2: Data Preprocessing\nHandle missing values, outliers, and normalize the data.\n\nfrom sklearn.model_selection import train_test_split\n\n# Fill missing values\ndata.fillna(data.mean(), inplace=True)\n\n# Split data into train and test sets\ntrain_data, test_data = train_test_split(data, test_size=0.2, random_state=42)"
  },
  {
    "objectID": "posts/post-with-code/index.html#step-3-feature-engineering",
    "href": "posts/post-with-code/index.html#step-3-feature-engineering",
    "title": "From Data to Model: Building an End-to-End Pipeline in Python",
    "section": "Step 3: Feature Engineering",
    "text": "Step 3: Feature Engineering\nTransform categorical variables and scale numerical features.\n\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\n\n# Define categorical and numerical features\ncategorical_features = ['category_column']\nnumerical_features = ['numeric_column']\n\n# Create transformers\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), numerical_features),\n        ('cat', OneHotEncoder(), categorical_features)\n    ])\n\n# Apply preprocessing\nX_train = preprocessor.fit_transform(train_data)\nX_test = preprocessor.transform(test_data)"
  },
  {
    "objectID": "posts/post-with-code/index.html#step-4-model-training",
    "href": "posts/post-with-code/index.html#step-4-model-training",
    "title": "From Data to Model: Building an End-to-End Pipeline in Python",
    "section": "Step 4: Model Training",
    "text": "Step 4: Model Training\nChoose a model and train it on the preprocessed data.\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Define the model\nmodel = RandomForestClassifier(random_state=42)\n\n# Train the model\nmodel.fit(X_train, train_data['target_column'])"
  },
  {
    "objectID": "posts/post-with-code/index.html#step-5-evaluation",
    "href": "posts/post-with-code/index.html#step-5-evaluation",
    "title": "From Data to Model: Building an End-to-End Pipeline in Python",
    "section": "Step 5: Evaluation",
    "text": "Step 5: Evaluation\nEvaluate the model’s performance using appropriate metrics.\n\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Make predictions\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nprint(accuracy_score(test_data['target_column'], y_pred))\nprint(classification_report(test_data['target_column'], y_pred))"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "From Data to Model: Building an End-to-End Pipeline in Python\n\n\n\n\n\n\ntutorial\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 18, 2025\n\n\nWangkai Zhu\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJan 15, 2025\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html#real-world-scenarios",
    "href": "posts/post-with-code/index.html#real-world-scenarios",
    "title": "From Data to Model: Building an End-to-End Pipeline in Python",
    "section": "Real World Scenarios",
    "text": "Real World Scenarios\nML pipelines are especially useful in scenarios where consistency, scalability and automation are crucial. It is a reproducible modeling schema that could be beneficial in numerous fields, finance, winery, and medicine to name a few. In finance, banks can use a trained model to predict whether a given customer can be approved a loan given his personal information. In healthcare, models can be used to predict disease progression based on patient records, etc."
  },
  {
    "objectID": "posts/post-with-code/index.html#conclusion",
    "href": "posts/post-with-code/index.html#conclusion",
    "title": "From Data to Model: Building an End-to-End Pipeline in Python",
    "section": "Conclusion",
    "text": "Conclusion\nBy following this guide, you gained a comprehensive understanding of how to:\n1. Design and implement an end-to-end machine learning pipeline.\n2. Optimize workflows for scalability and efficiency.\n3. Deploy models effectively for real-world applications.\nBuilding ML pipelines can get very simple and very difficult at the same time. It is a process that requires patience and practice. With each project, you will refine your skills and tackle more problems to make meaningful impact. Whether you are trying to build your first pipeline or catching up on the exam tomorrow, thank you for going through my blog and hope your pipelines always run smoothly!"
  }
]